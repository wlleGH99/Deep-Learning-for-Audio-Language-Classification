{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EeB2fEny9V3",
        "outputId": "884b4546-e117-4d62-a1aa-969fc116737b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code adapted from Professor Bargal's CS440 class challenge file\n",
        "\n"
      ],
      "metadata": {
        "id": "ppJ4l4hk65gY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#os.environ['OMP_NUM_THREADS'] = '1'\n",
        "\n",
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a9jfDXq6zEOG",
        "outputId": "4e4e6751-9526-4581-be80-b5ddd280963c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgJOK4YR8i2F",
        "outputId": "bb80abab-c7d0-4c4a-bafd-9c7198ef5c3a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "image = PIL.Image.open(\"/content/gdrive/MyDrive/CS583/Testing Images/Japanese/Japanese_3.png\") \n",
        "width, height = image.size\n",
        "print(width, height)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjFm2eEF4jzH",
        "outputId": "816b4b1d-d441-4ad9-8af8-17d5ac1e56dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "216 144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA_LIST = os.listdir('/content/gdrive/MyDrive/CS583/Training Images')\n",
        "#test_list = os.listdir('/content/gdrive/MyDrive/CS583/Testing Images')\n",
        "#DATASET_PATH  = '/content/gdrive/MyDrive/CS583/Training Images'\n",
        "#TEST_DIR =  '/content/gdrive/MyDrive/CS583/Testing Images'\n",
        "DATA_LIST = os.listdir('/content/gdrive/MyDrive/CS583/Alternative Train')\n",
        "test_list = os.listdir('/content/gdrive/MyDrive/CS583/Alternative Test')\n",
        "#test_list = os.listdir('/content/gdrive/MyDrive/CS583/Second Alternative Test')\n",
        "DATASET_PATH  = '/content/gdrive/MyDrive/CS583/Alternative Train'\n",
        "#TEST_DIR =  '/content/gdrive/MyDrive/CS583/Alternative Test'\n",
        "TEST_DIR =  '/content/gdrive/MyDrive/CS583/Second Alternative Test'\n",
        "IMAGE_SIZE    = (216, 144)\n",
        "NUM_CLASSES   = len(DATA_LIST)\n",
        "num_test = len(test_list)\n",
        "BATCH_SIZE    = 64 \n",
        "NUM_EPOCHS    = 29\n",
        "LEARNING_RATE = 0.0003"
      ],
      "metadata": {
        "id": "CBffmu48zH8h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(DATA_LIST)\n",
        "print(test_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1D8e2NJzax_",
        "outputId": "92a16bfd-1ee5-4cd3-fdd8-f634da50d1a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Japanese', 'Chinese', 'English', 'French', 'Korean', 'Spanish']\n",
            "['Korean', 'Japanese', 'French', 'Chinese', 'English', 'Spanish']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqG0rEA_zmVL",
        "outputId": "a30da0fb-b532-46bd-d0d4-d47acf0aac23"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Korean', 'Japanese', 'French', 'Chinese', 'English', 'Spanish']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,rotation_range=50,featurewise_center = True,\n",
        "                                   featurewise_std_normalization = True,width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,shear_range=0.25,zoom_range=0.1,\n",
        "                                   zca_whitening = False,channel_shift_range = 20,\n",
        "                                   horizontal_flip = False,vertical_flip = True,\n",
        "                                   validation_split = 0.2,fill_mode='constant')"
      ],
      "metadata": {
        "id": "zSwWi7oT05rH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches = train_datagen.flow_from_directory(DATASET_PATH,target_size=IMAGE_SIZE,\n",
        "                                                  shuffle=True,batch_size=BATCH_SIZE,\n",
        "                                                  subset = \"training\",seed=42,\n",
        "                                                  class_mode=\"categorical\")\n",
        "\n",
        "valid_batches = train_datagen.flow_from_directory(DATASET_PATH,target_size=IMAGE_SIZE,\n",
        "                                                  shuffle=True,batch_size=BATCH_SIZE,\n",
        "                                                  subset = \"validation\",\n",
        "                                                  seed=42,class_mode=\"categorical\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJbk88kz1PC4",
        "outputId": "f84681cd-26b7-404f-a501-429530daf9ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1832 images belonging to 6 classes.\n",
            "Found 457 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches.labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8TjflDU1b30",
        "outputId": "2bd4fcc6-5dec-48ee-9e4d-ec51562125d5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 5, 5, 5], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_batches.class_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2m8RrDG2hsk",
        "outputId": "88ba2e21-4642-4369-93b6-3c45a07f397d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Chinese': 0,\n",
              " 'English': 1,\n",
              " 'French': 2,\n",
              " 'Japanese': 3,\n",
              " 'Korean': 4,\n",
              " 'Spanish': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from keras.applications.resnet_v2 import ResNet101V2\n",
        "#sequential_1 = tf.keras.models.Sequential(ResNet101V2(include_top=False, weights=\"imagenet\", input_shape=(216,144,3)))\n",
        "#for layer in sequential_1.layers:\n",
        "#    layer.trainable = False"
      ],
      "metadata": {
        "id": "8C8fps4JBVqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df97b29-777d-4e82-8419-b612d3bda610"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171319296/171317808 [==============================] - 2s 0us/step\n",
            "171327488/171317808 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequential_1 = tf.keras.models.Sequential()\n",
        "sequential_1.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(216,144,3)))\n",
        "#sequential_1.add(tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n",
        "sequential_1.add(tf.keras.layers.MaxPooling2D(\n",
        "    pool_size=(3, 3), strides=None, padding=\"same\"))\n",
        "sequential_1.add(tf.keras.layers.Flatten())\n",
        "sequential_1.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "sequential_1.add(tf.keras.layers.Dropout(0.25))\n",
        "sequential_1.add(tf.keras.layers.Dense(6)) # 7 when the dataset is the original, 6 with the Tatoeba dataset\n",
        "sequential_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1oTFEq13Pvv",
        "outputId": "f0192dbf-b435-411f-8261-a1e3a64cbaf6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet101v2 (Functional)    (None, 7, 5, 2048)        42626560  \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 7, 5, 32)          589856    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 3, 2, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 192)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               49408     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 1542      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 43,267,366\n",
            "Trainable params: 640,806\n",
            "Non-trainable params: 42,626,560\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "STEP_SIZE_TRAIN=train_batches.n//train_batches.batch_size\n",
        "STEP_SIZE_VALID=valid_batches.n//valid_batches.batch_size\n",
        "\n",
        "sequential_1.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "model = sequential_1.fit(train_batches, validation_data=valid_batches, epochs=NUM_EPOCHS, steps_per_epoch=STEP_SIZE_TRAIN, \n",
        "                 validation_steps = STEP_SIZE_VALID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0n_BeRa5P_g",
        "outputId": "63863da7-dba5-4794-bf20-f6aa80025136"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/29\n",
            "28/28 [==============================] - 361s 13s/step - loss: 2.1779 - accuracy: 0.2393 - val_loss: 1.5571 - val_accuracy: 0.2969\n",
            "Epoch 2/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 1.3461 - accuracy: 0.4259 - val_loss: 1.2468 - val_accuracy: 0.5000\n",
            "Epoch 3/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 1.0142 - accuracy: 0.5979 - val_loss: 1.0763 - val_accuracy: 0.5938\n",
            "Epoch 4/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.8551 - accuracy: 0.6357 - val_loss: 0.9933 - val_accuracy: 0.6205\n",
            "Epoch 5/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.7487 - accuracy: 0.6861 - val_loss: 0.8386 - val_accuracy: 0.6786\n",
            "Epoch 6/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.6503 - accuracy: 0.7381 - val_loss: 0.8026 - val_accuracy: 0.7121\n",
            "Epoch 7/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.6323 - accuracy: 0.7523 - val_loss: 0.6687 - val_accuracy: 0.7321\n",
            "Epoch 8/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.5487 - accuracy: 0.7947 - val_loss: 0.6931 - val_accuracy: 0.7768\n",
            "Epoch 9/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.5305 - accuracy: 0.7913 - val_loss: 0.7763 - val_accuracy: 0.7277\n",
            "Epoch 10/29\n",
            "28/28 [==============================] - 30s 1s/step - loss: 0.5429 - accuracy: 0.7902 - val_loss: 0.6349 - val_accuracy: 0.7857\n",
            "Epoch 11/29\n",
            "28/28 [==============================] - 30s 1s/step - loss: 0.5028 - accuracy: 0.8060 - val_loss: 0.6360 - val_accuracy: 0.7545\n",
            "Epoch 12/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.4708 - accuracy: 0.8241 - val_loss: 0.6346 - val_accuracy: 0.7701\n",
            "Epoch 13/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.4211 - accuracy: 0.8422 - val_loss: 0.6010 - val_accuracy: 0.7902\n",
            "Epoch 14/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.4184 - accuracy: 0.8405 - val_loss: 0.6442 - val_accuracy: 0.8013\n",
            "Epoch 15/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.4483 - accuracy: 0.8286 - val_loss: 0.5968 - val_accuracy: 0.7812\n",
            "Epoch 16/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.4126 - accuracy: 0.8462 - val_loss: 0.4805 - val_accuracy: 0.8214\n",
            "Epoch 17/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.4064 - accuracy: 0.8569 - val_loss: 0.4778 - val_accuracy: 0.8281\n",
            "Epoch 18/29\n",
            "28/28 [==============================] - 30s 1s/step - loss: 0.4071 - accuracy: 0.8490 - val_loss: 0.4823 - val_accuracy: 0.8304\n",
            "Epoch 19/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.4101 - accuracy: 0.8433 - val_loss: 0.5443 - val_accuracy: 0.8125\n",
            "Epoch 20/29\n",
            "28/28 [==============================] - 30s 1s/step - loss: 0.3898 - accuracy: 0.8586 - val_loss: 0.4917 - val_accuracy: 0.8415\n",
            "Epoch 21/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.3743 - accuracy: 0.8609 - val_loss: 0.4675 - val_accuracy: 0.8170\n",
            "Epoch 22/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.3563 - accuracy: 0.8648 - val_loss: 0.5108 - val_accuracy: 0.8259\n",
            "Epoch 23/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.3901 - accuracy: 0.8580 - val_loss: 0.5331 - val_accuracy: 0.7946\n",
            "Epoch 24/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.3968 - accuracy: 0.8479 - val_loss: 0.4742 - val_accuracy: 0.8415\n",
            "Epoch 25/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.3185 - accuracy: 0.8812 - val_loss: 0.4123 - val_accuracy: 0.8348\n",
            "Epoch 26/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.2993 - accuracy: 0.8925 - val_loss: 0.5786 - val_accuracy: 0.8259\n",
            "Epoch 27/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.3272 - accuracy: 0.8767 - val_loss: 0.4740 - val_accuracy: 0.8415\n",
            "Epoch 28/29\n",
            "28/28 [==============================] - 29s 1s/step - loss: 0.2801 - accuracy: 0.9005 - val_loss: 0.4388 - val_accuracy: 0.8616\n",
            "Epoch 29/29\n",
            "28/28 [==============================] - 30s 1s/step - loss: 0.3422 - accuracy: 0.8772 - val_loss: 0.4504 - val_accuracy: 0.8304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "eval_generator = test_datagen.flow_from_directory(TEST_DIR,target_size=IMAGE_SIZE,\n",
        "                                                  batch_size=1,shuffle=True,seed=42,class_mode=\"categorical\")\n",
        "eval_generator.reset()\n",
        "print(len(eval_generator))\n",
        "print(eval_generator.class_indices)\n",
        "x = sequential_1.evaluate_generator(eval_generator,steps = np.ceil(len(eval_generator)),\n",
        "                           use_multiprocessing = False,verbose = 1,workers=1)\n",
        "print('Test loss:' , x[0])\n",
        "print('Test accuracy:',x[1])"
      ],
      "metadata": {
        "id": "wzX0nfR7_kNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec807b5-7ba3-4ae5-c653-df512e75d0b2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2401 images belonging to 6 classes.\n",
            "2401\n",
            "{'Chinese': 0, 'English': 1, 'French': 2, 'Japanese': 3, 'Korean': 4, 'Spanish': 5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2401/2401 [==============================] - 400s 166ms/step - loss: 2.9595 - accuracy: 0.2220\n",
            "Test loss: 2.9595024585723877\n",
            "Test accuracy: 0.22199083864688873\n"
          ]
        }
      ]
    }
  ]
}